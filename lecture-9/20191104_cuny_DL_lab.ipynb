{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lT63quvgri0L"
   },
   "source": [
    "* The empty ipynb for you to start from in Grant's repo: https://github.com/grantmlong/itds-fall2019/tree/master/lecture-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTApi-7voeCo"
   },
   "source": [
    "# Setting the stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fP6QeXnVeWzi",
    "outputId": "4b1a109d-f35d-4a33-b758-bff725a5dcf3"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "fHza7chYecM4",
    "outputId": "0ec0e972-728b-447b-a5ba-91a9bbe9dada"
   },
   "outputs": [],
   "source": [
    "# if running on colab, pytorch is already installed.\n",
    "# if running locally, conda or pip install this in your conda environment:\n",
    "# conda install pytorch torchvision -c pytorch\n",
    "# OR\n",
    "# pip3 install torch torchvision\n",
    "\n",
    "# I'll be assuming python >=3.6 and torch >=1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ropMkTLTiWos",
    "outputId": "9cba56ae-1b3c-4bd1-f85f-f9c7e83fa9ed"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgfEx8YN0BDF"
   },
   "source": [
    "# Torch and autograd basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch is a package that defines vectors, matrices, or in general \"tensors\". If you know numpy, you will not be surprised by any of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmvFvWHJ0GQ1"
   },
   "outputs": [],
   "source": [
    "a = torch.ones(3,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mur2vsHp0GmX"
   },
   "outputs": [],
   "source": [
    "b = torch.arange(9).float().view(3,3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hQiSHvW0G2v"
   },
   "outputs": [],
   "source": [
    "(a+b)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.zero_() # operations with an underscore modify the Tensor in place.\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can slice and dice tensors and they have roughly all tensor operations you expect equivalently to numpy, but with a bit more low level control. If you need more intro: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "\n",
    "So what's the big deal about pytorch?\n",
    "\n",
    "**autograd = automatic differentiation**.\n",
    "\n",
    "Every `torch.Tensor`, let's say `x`, has an important flag `requires_grad`. If this flag is set to True, pytorch will keep track of the graph of operations that happen with this tensor.\n",
    "When we finally arrive at some output (a scalar variable based on a sequence of operations on `x`), we can call `.backward()` on this output, to compute the gradient `d(output) / dx`. This gradient will end up in `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=(x**2 + x)\n",
    "z = y.sum()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from high school math that the derivative `dz / dx[i,j]` = 2*x +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*x+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the intermediate variable y? Does it require a gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the gradient of y is not exposed, since it is an intermediary variable, the result of an operation on leaf variables. Leaf variables are inputs to the operations: the data X or the `Parameter`s of a neural network.\n",
    "\n",
    "More about autograd in the tutorial https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py and the docs https://pytorch.org/docs/stable/autograd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we talked about how derivatives and backpropagation (based on the chain rule of differentiation) play a central role in deep learning. You can now start to see how this autograd will be massively powerful to define neural networks with weights `W` and optimize them based on the gradients in `W.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression on a toy problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this for a simple linear mapping `y = f(x, theta) = <x, theta>` with $x, \\theta \\in \\mathbb{R}^{2}$. We we want to optimize $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(23231)\n",
    "x1 = torch.Tensor([1, 2, 3, -3, -2])\n",
    "y = torch.Tensor ([3, 6, 9, -9, -6]).view(5,1)\n",
    "x2 = torch.randn(5)\n",
    "x = torch.stack([x1, x2], dim=1) # 5 x 2 input. 5 datapoints, 2 dimensions.\n",
    "# theta = torch.randn(1,2, requires_grad=True) # ~equal to:\n",
    "theta = torch.nn.Parameter(torch.randn(1,2))\n",
    "# we start theta at random initialization, the gradient will point us in the right direction.\n",
    "print('x:\\n', x)\n",
    "print('y:\\n', y)\n",
    "print('theta at random initialization: ', theta)\n",
    "thetatrace = [theta.data.clone()] # initial value, for logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at x and y. What is their correct (linear) relationship?\n",
    "\n",
    "A: `y = 3 x1 + 0 x2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a prediction as the inner product $\\hat{y} = \\langle x , \\theta\\rangle$ - implemented as simple matrix multiply with python3's `@` operator.\n",
    "\n",
    "We will compute the ordinary least squares objective (mean squared error):  $\\mathcal{L}(\\theta) = (\\hat{y}(x,\\theta) - y)^2$\n",
    "\n",
    "Compute $\\nabla_\\theta \\mathcal{L}(\\theta)$, and\n",
    "\n",
    "Move $\\theta$ a small step opposite to that direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = x @ theta.t() # matrix multiply; (N x 2) * (2 x 1) -> N x 1\n",
    "print('ypred:\\n', ypred)\n",
    "loss = ((ypred-y)**2).mean() # mean squared error = MSE\n",
    "print('mse loss: ', loss.item())\n",
    "loss.backward()\n",
    "print('dL / d theta:\\n', theta.grad)\n",
    "# let's move W in that direction\n",
    "theta.data.add_(-0.1 * theta.grad.data)\n",
    "# Now we will reset the gradient to zero.\n",
    "theta.grad.zero_()\n",
    "print('theta:\\n', theta)\n",
    "thetatrace.append(theta.data.clone()) # for logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can re-execute this cell above a couple of times and see how W goes close towards the optimal value of `[3,0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us plot in 2D what happened to theta during SGD optimization. In red is the true relation.\n",
    "thetas = torch.cat(thetatrace, dim=0).numpy()\n",
    "plt.figure()\n",
    "plt.plot(thetas[:,0], thetas[:, 1], 'x-')\n",
    "plt.plot(3, 0, 'ro')\n",
    "plt.xlabel('theta[0]')\n",
    "plt.ylabel('theta[1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w00A5MF07pIw"
   },
   "source": [
    "Ok, doing this manually gives you insight what happens down to the details. But usually we do not do the gradient updates manually, it would become very cumbersome if the net becomes more complex than the simple linear layer. pytorch gives us abstractions to easily manage this complexity: \n",
    "* `nn.Linear()` (or generally  `Module`s) which do two things: (a) they contain the learnable weight, and (b) define how they operate on an input tensor to give an output.\n",
    "* `module.zero_grad()` to clear the gradients, \n",
    "* `optim.SGD` with which you can do `optimizer.step()` to do a step of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(23801)\n",
    "net = nn.Linear(2,1, bias=False)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1) # do updates with `optimizer.step()`\n",
    "# x, y defined above. In a real problem we would typically get different x, y \"minibatches\"\n",
    "# of samples from a dataloader.\n",
    "for i in range(100): # 10 optimization steps (gradient descent steps)\n",
    "    ypred = net(x)\n",
    "    loss = ((ypred-y)**2).mean() # mean squared error = MSE\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # and instead of W.data -= 0.1 * W.grad we do:\n",
    "    optimizer.step()\n",
    "print(net.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0y1lGv8oIHI",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Revisiting KIVA logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSygYcyKwTf6"
   },
   "source": [
    "## sklearn solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "tx6mA3tJo3DE",
    "outputId": "e70a0c66-f484-4b05-ce71-01a0dd9344c3"
   },
   "outputs": [],
   "source": [
    "# if you downloaded the kiva data locally, you can skip wget and just set the path\n",
    "!wget https://grantmlong.com/data/kiva_kenya_sample.csv\n",
    "! curl -O https://grantmlong.com/data/kiva_kenya_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "72VlJgMjo3v9",
    "outputId": "af4e9a1f-d6b5-433a-d047-28235ebf50dc"
   },
   "outputs": [],
   "source": [
    "# So here i copy pasted the lab 6/7 logistic regression code where we leave the\n",
    "# model fitting to sklearn.\n",
    "data_path = 'kiva_kenya_sample.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "print(df.shape)\n",
    "print(list(df))\n",
    "df['success'] = (df.STATUS=='funded')*1\n",
    "df['posted_year'] = pd.to_datetime(df.POSTED_TIME).dt.year\n",
    "df['posted_duration'] = (pd.to_datetime(df.PLANNED_EXPIRATION_TIME)\n",
    "                             - pd.to_datetime(df.POSTED_TIME)\n",
    "                            ).dt.days\n",
    "model_columns = ['LOAN_AMOUNT', 'posted_year', 'posted_duration', 'LENDER_TERM']\n",
    "valids = df[model_columns].notna().all(axis=1)\n",
    "X = df.loc[valids, model_columns].values\n",
    "X_scaled = preprocessing.scale(X) # zero mean, unit variance\n",
    "\n",
    "y = df.loc[valids, 'success'].values\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=1235)\n",
    "\n",
    "# OK data is ready, set up the logistic regression classifier and train it.\n",
    "clf = LogisticRegression(random_state=20181022, multi_class='multinomial', solver='lbfgs')\n",
    "clf.fit(X_train_np, y_train_np)\n",
    "W, b = clf.coef_, clf.intercept_\n",
    "print('The train accuracy of the sklearn model is %0.1f%%' % (clf.score(X_train_np, y_train_np)*100))\n",
    "print('The test  accuracy of the sklearn model is %0.1f%%' % (clf.score(X_test_np, y_test_np)*100))\n",
    "print('The learned model: y=W*x+b where \\nW={} and \\nb={}'.format(W,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJwW0C5Yw1Rg"
   },
   "source": [
    "* How many parameters (weights) does our logistic regression model have? How many  datapoints did we train on?\n",
    "* Old-skool machine learning rule of thumb is: you can optimize about as many parameters (weights) as you have datapoints before you can memorize the dataset (thus overfit heavily). Are we close to the limit?\n",
    "* Does the model overfit?\n",
    "* In deep neural networks you can easily have way more parameters than datapoints. Is overfitting an issue for neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2-BhVGTvOTM"
   },
   "outputs": [],
   "source": [
    "# number of parameters is defined by weight and bias dimensionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BgYdSOCXwc0j"
   },
   "source": [
    "## Torch implementation of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LaHzzHci0m5i"
   },
   "source": [
    "Ok so above we used the sklearn implementation for logistic regression, which optimizes  $$\\mathcal{L(W,b)} = \\sum_{x,y_{t} \\in D} \\ell(x,y_t; W,b)$$\n",
    "with $$\\ell(x,y_t; W,b) = \\text{CE}(\\sigma(Wx+b) || y_t)$$\n",
    "So per sample, the sigmoid of the linear transformation $\\sigma(Wx+b)$ is the model's prediction of the probability.  This is being compared to the true label $y_t$,\n",
    "by the Cross-Entropy loss: $\\text{CE}(\\sigma(Wx+b) || y_t)$.\n",
    "\n",
    "Now we'll set up a small neural network, and optimize the same loss with SGD. We will compare the solution to the one found by sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the typical training procedure for a neural network which is as follows:\n",
    "\n",
    "* Define the neural network that has some learnable parameters (or weights)\n",
    "* Iterate over a dataset of inputs\n",
    "* Process input through the network\n",
    "* Compute the loss (how far is the output from being correct)\n",
    "* Propagate gradients back into the network’s parameters\n",
    "* Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zr4H1--I3ohW"
   },
   "outputs": [],
   "source": [
    "X_train, X_test = torch.from_numpy(X_train_np).float(), torch.from_numpy(X_test_np).float()\n",
    "y_train, y_test = torch.from_numpy(y_train_np).int(),   torch.from_numpy(y_test_np).int()\n",
    "# we will define a \"neural network\" of 1 layer:\n",
    "torch.manual_seed(1238)\n",
    "net = nn.Linear(4,1) # computes W X + b where X is a (batch of) 4D vectors and y will be 1D.\n",
    "sigmoid = nn.Sigmoid()\n",
    "loss = nn.BCELoss() # Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhT6DQenyJOE"
   },
   "source": [
    "Print the value of the weight and the bias of the network. Do they have any gradients right now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_n2F1zuzyJUG"
   },
   "outputs": [],
   "source": [
    "# For most modules, they're called .weight and .bias\n",
    "# gradients are an attribute of a Parameter (weight) and are accessible in .grad\n",
    "print(net.weight)\n",
    "print(net.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "S851ry0z4Mls",
    "outputId": "50ebecd1-b169-4b15-a7a3-0012dc8f01a0"
   },
   "outputs": [],
   "source": [
    "def print_accs(net, s=''):\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    # net(x) is continuous value, threshold at 0 to make binary prediction\n",
    "    pred_train = (net(X_train) > 0).squeeze().int()\n",
    "    acc_train = (pred_train == y_train).float().mean()\n",
    "    print('Train accuracy is %0.1f%%  - %s' % (acc_train*100, s))\n",
    "    net.eval()\n",
    "    pred_test = (net(X_test) > 0).squeeze().int()\n",
    "    acc_test = (pred_test == y_test).float().mean()\n",
    "    print('Test  accuracy is %0.1f%%  - %s' % (acc_test*100, s))\n",
    "    net.train()\n",
    "# let's print the accuracies for the untrained net, which will be random guess\n",
    "print_accs(net, 'just initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn hides everything from you. Let's copy the weights into a torch network\n",
    "# and compute accuracies ourselves.\n",
    "refnet = nn.Linear(4,1)\n",
    "W, b = clf.coef_, clf.intercept_\n",
    "refnet.weight.data.copy_(torch.from_numpy(W))\n",
    "refnet.bias.data.copy_(torch.from_numpy(b))\n",
    "print(refnet.weight) # from the sklearn optimized model\n",
    "print_accs(refnet, 'sklearn weights copied over into refnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNjVlTt87ou1"
   },
   "source": [
    "Now we'll optimize the weights and bias with SGD. Dataset and DataLoader are abstractions to help us iterate over the data in random order. We will do 1 epoch, i.e. we go through the data only once, in minibatches of size 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "nIhpH0sN7o5m",
    "outputId": "2c3207e0-8da8-4053-ac73-17fe5eef07f2"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1238)\n",
    "net = nn.Linear(4,1) # re initialize the net from scratch\n",
    "print('Just initialized: weight: ', net.weight, '\\nbias: ', net.bias)\n",
    "w, b = net.weight, net.bias \n",
    "lr = 1.0\n",
    "dl = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "print_accs(net, 'before epoch')\n",
    "for x,ytarget in dl:\n",
    "    ypred = sigmoid(net(x).squeeze(1)) # forward, pytorch constructs the graph\n",
    "    output = loss(ypred, ytarget.float()) # forward, pytorch constructs the graph\n",
    "    output.backward() # backward, pytorch computes net.weight.grad and net.bias.grad\n",
    "    # access to the weight & bias tensors outside of pytorch autograd\n",
    "    w, grad_w, b, grad_b = net.weight.data, net.weight.grad.data, net.bias.data, net.bias.grad.data\n",
    "    # TODO: do an SGD step for both weight and bias: w -= lr * grad\n",
    "    # TODO: manually clear the gradient of the weight and the bias (use `.zero_()` method) \n",
    "    # we need to do this before doing the forward and backward pass.\n",
    "print_accs(net, 'after epoch')\n",
    "print('weight: ', net.weight, '\\nbias: ', net.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w00A5MF07pIw"
   },
   "source": [
    "Ok doing this manually gives you insight what happens down to the gradienst. But usually we do not do these things manually, it would become very cumbersome if the net becomes more complex than the simple linear layer. pytorch gives us primitives to do the same: `net.zero_grad()` to clear the gradients, and for optimization you can do `optimizer.step()` to do a step of SGD.\n",
    "Again we will do 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oxs6aggO7pQH"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1238)\n",
    "net = nn.Linear(4,1) # re initialize the net from scratch\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "dl = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "print_accs(net, 'before epoch')\n",
    "for x,ytarget in dl:\n",
    "    ypred = sigmoid(net(x).squeeze(1))\n",
    "    output = loss(ypred, ytarget.float())\n",
    "    output.backward()\n",
    "    # TODO use the gradients to do a step (use the optimizer)\n",
    "    # TODO clear the gradient\n",
    "print_accs(net, 'after epoch')\n",
    "print('weight: ', net.weight, '\\nbias: ', net.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tBv23he7pXP"
   },
   "source": [
    "Ok now let us redo this but for a real 3-layer neural network. You can re-execute the cell a couple of times to do more iterations and see the accuracy improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1248)\n",
    "net = nn.Sequential(\n",
    "    # first layer\n",
    "    nn.Linear(4,32),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,32),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.ReLU(),\n",
    "    # output layer going to 1 prediction\n",
    "    nn.Linear(32,1),\n",
    ")\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "dl = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ajv2kGVU7peF"
   },
   "outputs": [],
   "source": [
    "# Do 1 epoch:\n",
    "print_accs(net, 'before epoch')\n",
    "for x,ytarget in dl:\n",
    "    ypred = sigmoid(net(x).squeeze(1))\n",
    "    output = loss(ypred, ytarget.float())\n",
    "    output.backward()\n",
    "    optimizer.step()\n",
    "    net.zero_grad()\n",
    "print_accs(net, 'after epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a more typical complete loop, we train for 10 epochs and cut the learning rate in half between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-6-LmwMufPD"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1248)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4,16),\n",
    "#     nn.Dropout(0.1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16,32),\n",
    "#     nn.Dropout(0.1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,16),\n",
    "    nn.ReLU(),\n",
    "    # output layer going to 1 prediction\n",
    "    nn.Linear(16,1),\n",
    ")\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "dl = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "print_accs(net, 'Before training')\n",
    "for epoch in range(10):\n",
    "    for x,ytarget in dl:\n",
    "        ypred = sigmoid(net(x).squeeze(1))\n",
    "        output = loss(ypred, ytarget.float())\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "        net.zero_grad()\n",
    "    print_accs(net, 'After epoch {}'.format(epoch))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila and that's how it's done. You can ask yourself some more questions:\n",
    "* What do we get back from `net.parameters()`: which trainable weights and biases does the network have now? \n",
    "* How many total parameters? \n",
    "* What happens if you add layers or change the ReLU activation by Sigmoid activation?\n",
    "* What does the Dropout layer do? \n",
    "\n",
    "There are many things you can play around with and where you can dig deeper.\n",
    "\n",
    "Note that I'm not claiming that sklearn actually does this (SGD) optimization inside - in fact it is using much more complex/appropriate convex optimization methods, see the `solver` argument to sklearn LogisticRegrsssion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdh_P4DKoNup",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Now the real stuff: MNIST classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a dataset of 50k handwritten digits (0-9) which is very commonly used in the deep learning community.\n",
    "It is small enough to work with locally and without much hassle, and complex enough to do something interesting with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "PH8zthzBhjKS",
    "outputId": "d959308e-faa5-4912-a44e-d5be1001b65b"
   },
   "outputs": [],
   "source": [
    "# let's download the MNIST data, if you do this locally and you downloaded before,\n",
    "# you can change data paths to point to your existing files\n",
    "train_dataset = dsets.MNIST(root='./MNISTdata',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./MNISTdata',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the digits and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix=129\n",
    "x,y = train_dataset[ix]\n",
    "plt.imshow(x.squeeze().numpy())\n",
    "print('label: y={}'.format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the dataloaders and train simple neural network like before.\n",
    "You'll recognize that the core is exactly the same: we do a forward pass, compute a loss, backpropagate the loss to compute the gradients, then let the optimizer update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tONXsS3FtvFK"
   },
   "outputs": [],
   "source": [
    "# The neural network hyperparameters.\n",
    "input_size    = 784   # The MNIST image size = 28 x 28 = 784\n",
    "hidden_size   = 100   # The number of nodes at the hidden layer\n",
    "num_classes   = 10    # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs    = 5     # The number of times entire dataset is trained\n",
    "batch_size    = 100   # The number of samples per minibatch\n",
    "learning_rate = 1.0   # SGD step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "colab_type": "code",
    "id": "UZoB9O6ce0js",
    "outputId": "b20332d2-7abd-42f5-f788-452591dccdde"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define and train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define simple MLP network. Train network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "        self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        x = x.view(x.size(0), -1) # flatten (bs x 1 x 28 x 28) -> (bs x 784)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "def test(net, dl):\n",
    "    right, tot = 0, 0\n",
    "    net.eval() # Set dropout and possibly other modules in eval mode.\n",
    "    for x,y in dl:\n",
    "        ypred = net(x).argmax(dim=1) # select index of maximal score\n",
    "        right += (ypred == y).sum().item()\n",
    "        tot   += x.size(0)\n",
    "    return 1.* right / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') # if on gpu-enabled machine, set torch.device('cuda')\n",
    "# create the net based on this class definition\n",
    "net = Net(input_size, hidden_size, num_classes).to(device)\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "print('Before training: {:.1f}% test accuracy'.format(100*test(net, test_loader)))\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (x,y_target) in enumerate(train_loader):\n",
    "        y_probs = F.log_softmax(net(x), dim=1) # NOTE \n",
    "        output = F.nll_loss(y_probs, y_target)\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "        net.zero_grad()\n",
    "    print('After epoch {}: {:.1f}% test accuracy'.format(epoch, 100*test(net, test_loader)))\n",
    "print('End of training: {:.1f}% train accuracy'.format(100*test(net, train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some questions for you to investigate:\n",
    "* what does softmax do? (test on a random vector)\n",
    "* what is its purpose? (read the docs)\n",
    "* what does nll_loss do? can you manually compute it?\n",
    "* Make sure to use `log_softmax()` insead of `softmax()`,\n",
    "which is what nll_loss expects:\n",
    "https://pytorch.org/docs/master/nn.html#torch.nn.functional.nll_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the prediction on some samples.\n",
    "ix=1234\n",
    "x,y = test_dataset[ix]\n",
    "plt.imshow(x.squeeze().numpy())\n",
    "print('Ground truth label: y={}'.format(y))\n",
    "y_probs = F.softmax(net(x), dim=1)\n",
    "print ('Model probabilities: ') \n",
    "print(' / '.join(['{}: {:.3f}'.format(k,v) \n",
    "                  for k,v in zip(range(10), y_probs.squeeze().tolist()) ] ))\n",
    "print('Model prediction: ', y_probs.argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we used a simple flat neural network which looks at the image as a flat vector, without awareness of the 2D structure or which pixels neighbor each other.\n",
    "A convolutional neural network is an architecture that takes the 2D structure of the image into account by sliding a kernel over all the different locations in the image. This kind of neural network has been very succesful in image recognition [1] and speech recognition [2,3].\n",
    "Pytorch and other deep learning toolboxes are designed to deal with this kind of data and with convolutional neural networks just as easily as with flat data. Try swapping out the network above for a convolutional neural network, see for example the pytorch tutorial [4].\n",
    "\n",
    "[1] https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\n",
    "[2] http://www.cs.toronto.edu/~asamir/papers/icassp13_cnn.pdf\n",
    "[3] https://arxiv.org/abs/1509.08967\n",
    "[4] https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "[5] https://colab.research.google.com/drive/1jxUPzMsAkBboHMQtGyfv5M5c7hU8Ss2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48JIxr6HoXH6"
   },
   "source": [
    "# Finishing notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUsyDpADjBbI"
   },
   "source": [
    "Inspiration for this lab and the lecture:\n",
    "\n",
    "*  An old lab I made in lua torch https://github.com/tomsercu/torchtutorial\n",
    "* This pytorch intro notebook https://colab.research.google.com/drive/1jxUPzMsAkBboHMQtGyfv5M5c7hU8Ss2c\n",
    "* The official pytorch tutorial https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "* Yann LeCuns deep learning course in 2015 https://cilvr.nyu.edu/doku.php?id=deeplearning2015:schedule\n",
    "* This GAN tutorial https://github.com/tomsercu/gan-tutorial-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJog1HK1nRf7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jdh_P4DKoNup",
    "48JIxr6HoXH6"
   ],
   "name": "20181203_cuny_DL_lab",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
